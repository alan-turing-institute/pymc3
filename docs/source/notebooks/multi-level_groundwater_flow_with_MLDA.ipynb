{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilevel Groundwater Flow with MLDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLDA sampler\n",
    "This notebook (along with the utility code within the `./mlda` folder) is designed to demonstrate the Multi-Level Delayed Acceptance MCMC algorithm (MLDA) proposed in [1], as implemented within pymc3. \n",
    "\n",
    "The MLDA sampler can be more efficient than other MCMC samplers when dealing with computationally intensive problems where we have access not only to the desired (fine) posterior distribution but also to a set of approximate (coarse) posteriors of decreasing accuracy and decreasing computational cost. \n",
    "\n",
    "This is the case e.g. in subsurface flow models where a partial differential equation (PDE) with highly varying coefficients needs to be solved numerically on a fine spatial grid to perform each MCMC likelihood computation. If we have access to versions of the same model on coarser grids, we can apply a multilevel approach; in simple terms, we can use multiple chains on different coarseness levels and coarser chains' samples are used as proposals for the finer chains. This has been shown to improve the effective sample size of the finest chain and this allows us to reduce the number of expensive fine-chain likelihood evaluations.\n",
    "\n",
    "For more details about the MLDA sampler and the way it should be used and parameterised, the user can refer to the code below, as well as the docstrings within the python code (the implementation is under `pymc3/pymc3/step_methods/metropolis.py`).\n",
    "\n",
    "Please note that the MLDA sampler is new in pymc3. The user should be extra critical about the results and report any problems as issues in the pymc3's github repository.\n",
    "\n",
    "### The model\n",
    "Within this notebook, a simple MLDA sampler is compared to pymc3's Metropolis MCMC sampler. The target posterior is defined within the context of a Bayesian inverse problem for groundwater flow modeling, where we solve a PDE in each likelihood evaluation and we are able to do this in different coarseness levels.\n",
    "\n",
    "The example demonstrates that MLDA is more efficient than Metropolis when measured by the Effective Samples per Second they can generate from the posterior. MLDA's relative efficiency becomes even larger when the size of the problem (i.e. fine resolution, number of parameters) increases. The user is encouraged to try different sizes, since the\n",
    "ones used here are moderate to avoid long runtimes. Note that the likelihood in this example cannot be automatically differentiated therefore NUTS cannot be used.\n",
    "\n",
    "### PDE solver details\n",
    "The code within the `./mlda` folder solves the steady state groundwater flow problem for a random hydraulic conductivity field [2]. This solution acts as the forward model in the context of a Bayesian Inverse problem. In conjunction with MCMC, this setup allows for sampling from the posterior distribution of model parameters (hydraulic conductivity) given data (measurements of hydraulic head). This setup has applications in hydrogeological parameter estimation and uncertainty quantification. The forward model can work in differing levels of coarseness, allowing us to use a multilevel approach. \n",
    "\n",
    "Note that the solver uses \"black box\" code from an external C++ library (FEniCS [3]) which needs to be installed before running the notebook.\n",
    "\n",
    "The files within `./mlda` contain the following:\n",
    "\n",
    "`GwFlow.py`: This module is a FEniCS implementation of steady state groundwater flow in a confined aquifer, an elliptic Partial Differential Equation (PDE). It creates a mesh on a unit square domain, given the input resolution, imposes fixed head boundary conditions of 1 and 0 at left and right boundaries, respectively, and no-flow boundary conditions at the top and bottom boundaries. It then solves the problem given some FEniCS [3] function, representative of heterogeneous, isotropic hydraulic conductivity.\n",
    "\n",
    "`RandomProcess.py`: This module generates realisations of a Gaussian Random Field with a squared exponential covariance function, as described in [2], representative of the (unknown) aquifer conductivity field. It takes a n x d matrix of nodal coordinates, a covariance length scale and a truncation parameter k, and computes the k largest Karhunen–Loève (KL) eigenmodes of the covariance matrix. It then generates random field realisations according to the mean and standard deviation of the desired log-conductivity field and a vector of random parameters.\n",
    "\n",
    "`Model.py`: A module that wraps the groundwater flow solver and random process into one object. When the solve-method is called, it will solve the groundwater flow problem given a vector of random field parameters. The user can then extract hydraulic heads at datapoints using the get_data-method.\n",
    "\n",
    "\n",
    "### Dependencies\n",
    "The code has been developed and tested with Python 3.6. You will need to have pymc3 installed and also install [FEniCS](https://fenicsproject.org/) for your system.\n",
    "  \n",
    "\n",
    "### References\n",
    "[1] Dodwell, Tim & Ketelsen, Chris & Scheichl, Robert & Teckentrup, Aretha. (2019). Multilevel Markov Chain Monte Carlo. SIAM Review. 61. 509-545. https://doi.org/10.1137/19M126966X\n",
    "\n",
    "[2] Scarth, C., Adhikari, S., Cabral, P. H., Silva, G. H. C., & Prado, A. P. do. (2019). Random field simulation over curved surfaces: Applications to computational structural mechanics. Computer Methods in Applied Mechanics and Engineering, 345, 283–301. https://doi.org/10.1016/j.cma.2018.10.026\n",
    "\n",
    "[3] The FEniCS Project Version 1.5 M. S. Alnaes, J. Blechta, J. Hake, A. Johansson, B. Kehlet, A. Logg, C. Richardson, J. Ring, M. E. Rognes and G. N. Wells, Archive of Numerical Software, vol. 3, 2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import groundwater flow model utils\n",
    "import sys\n",
    "sys.path.insert(1, 'mlda/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "from Model import Model, model_wrapper, project_eigenpairs\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Set environmental variable\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the resolution of the multi-level models (from coarsest to finest)\n",
    "# This is a list of different model resolutions. Each\n",
    "# resolution added to the list will add one level to the multi-level\n",
    "# inference. Each element is a tuple (x,y) where x, y are the number of \n",
    "# points in each dimension. For example, setting resolutions = \n",
    "# [(2,2), (4,4)] creates a coarse 2x2 model and a fine 4x4 model.\n",
    "resolutions = [(30, 30), (120, 120)]\n",
    "\n",
    "# Set random field parameters\n",
    "field_mean = 0\n",
    "field_stdev = 1\n",
    "lamb_cov = 0.1\n",
    "\n",
    "# Set the number of unknown parameters (i.e. dimension of theta in posterior)\n",
    "nparam = 3\n",
    "\n",
    "# Number of draws from the distribution\n",
    "ndraws = 1000\n",
    "\n",
    "# Number of burn-in samples\n",
    "nburn = 500\n",
    "\n",
    "# MLDA and Metropolis tuning parameters\n",
    "tune = True\n",
    "tune_interval = 100\n",
    "discard_tuning = True\n",
    "\n",
    "# Number of independent chains\n",
    "nchains = 2\n",
    "\n",
    "# Subsampling rate for MLDA\n",
    "nsub = 5\n",
    "\n",
    "# Do blocked/compounds sampling in Metropolis and MLDA \n",
    "# Note: This choice applies only to the coarsest level in MLDA \n",
    "# (where a Metropolis sampler is used), all other levels use block sampling\n",
    "blocked = False\n",
    "\n",
    "# Set the sigma for inference\n",
    "sigma = 0.01\n",
    "\n",
    "# Data generation seed\n",
    "data_seed = 12345\n",
    "\n",
    "# Sampling seed\n",
    "sampling_seed = 12345\n",
    "\n",
    "# Datapoints list\n",
    "points_list = [0.1, 0.3, 0.5, 0.7, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Theano Op along with the code within ./mlda to construct the likelihood\n",
    "\n",
    "def my_loglik(my_model, theta, datapoints, data, sigma):\n",
    "    \"\"\"\n",
    "    This returns the log-likelihood of my_model given theta,\n",
    "    datapoints, the observed data and sigma. It uses the\n",
    "    model_wrapper function to do a model solve.\n",
    "    \"\"\"\n",
    "    output = model_wrapper(my_model, theta, datapoints)\n",
    "    return - (0.5 / sigma ** 2) * np.sum((output - data) ** 2)\n",
    "\n",
    "class LogLike(tt.Op):\n",
    "    \"\"\"\n",
    "    Theano Op that wraps the log-likelihood computation, necessary to\n",
    "    pass \"black-box\" fenics code into pymc3.\n",
    "    Based on the work in:\n",
    "    https://docs.pymc.io/notebooks/blackbox_external_likelihood.html\n",
    "    https://docs.pymc.io/Advanced_usage_of_Theano_in_PyMC3.html\n",
    "    \"\"\"\n",
    "\n",
    "    # Specify what type of object will be passed and returned to the Op when it is\n",
    "    # called. In our case we will be passing it a vector of values (the parameters\n",
    "    # that define our model and a model object) and returning a single \"scalar\"\n",
    "    # value (the log-likelihood)\n",
    "    itypes = [tt.dvector]  # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self, my_model, loglike, data, x, sigma):\n",
    "        \"\"\"\n",
    "        Initialise the Op with various things that our log-likelihood function\n",
    "        requires. Below are the things that are needed in this particular\n",
    "        example.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        my_model:\n",
    "            A Model object (defined in model.py) that contains the parameters\n",
    "            and functions of out model.\n",
    "        loglike:\n",
    "            The log-likelihood function we've defined, in this example it is\n",
    "            my_loglik.\n",
    "        data:\n",
    "            The \"observed\" data that our log-likelihood function takes in. These\n",
    "            are the true data generated by the finest model in this example.\n",
    "        x:\n",
    "            The dependent variable (aka 'x') that our model requires. This is\n",
    "            the datapoints in this example.\n",
    "        sigma:\n",
    "            The noise standard deviation that our function requires.\n",
    "        \"\"\"\n",
    "\n",
    "        # add inputs as class attributes\n",
    "        self.my_model = my_model\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "        self.x = x\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        theta = inputs  # this will contain my variables\n",
    "\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(self.my_model, theta, self.x, self.data, self.sigma)\n",
    "\n",
    "        outputs[0][0] = np.array(logl) # output the log-likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model objects and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this can take several minutes for large resolutions\n",
    "my_models = []\n",
    "for r in resolutions:\n",
    "    my_models.append(Model(r, field_mean, field_stdev, nparam, lamb_cov))\n",
    "\n",
    "# Project eignevactors from fine model to all coarse models\n",
    "for i in range(len(my_models[:-1])):\n",
    "    project_eigenpairs(my_models[-1], my_models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve finest model as a test and plot transmissivity field and solution\n",
    "np.random.seed(data_seed)\n",
    "my_models[-1].solve()\n",
    "my_models[-1].plot(lognormal=False)\n",
    "\n",
    "# Save true parameters of finest model\n",
    "true_parameters = my_models[-1].random_process.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sampling points.\n",
    "x_data = y_data = np.array(points_list)\n",
    "datapoints = np.array(list(product(x_data, y_data)))\n",
    "\n",
    "# Get data from the sampling points and perturb it with some noise.\n",
    "noise = np.random.normal(0, 0.001, len(datapoints))\n",
    "\n",
    "# Generate data from the finest model for use in pymc3 inference - these data are used in all levels\n",
    "data = model_wrapper(my_models[-1], true_parameters, datapoints) + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate LogLik objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Theano Ops to wrap likelihoods of all model levels and store them in list\n",
    "logl = []\n",
    "for m in my_models:\n",
    "    logl.append(LogLike(m, my_loglik, data, datapoints, sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct pymc3 model objects for coarse models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models in pymc3 for each level - excluding finest model level\n",
    "coarse_models = []\n",
    "for j in range(len(my_models) - 1):\n",
    "    with pm.Model() as model:\n",
    "        # uniform priors on parameters\n",
    "        parameters = []\n",
    "        for i in range(nparam):\n",
    "            parameters.append(pm.Uniform('theta_' + str(i), lower=-3., upper=3.))\n",
    "\n",
    "        # convert m and c to a tensor vector\n",
    "        theta = tt.as_tensor_variable(parameters)\n",
    "\n",
    "        # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
    "        ll = logl[j]\n",
    "        pm.DensityDist('likelihood', lambda v: ll(v), observed={'v': theta})\n",
    "\n",
    "    coarse_models.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference using MLDA and Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up finest model and perform inference with PyMC3, using the MLDA algorithm\n",
    "# and passing the coarse_models list created above.\n",
    "method_names = []\n",
    "traces = []\n",
    "runtimes = []\n",
    "acc = []\n",
    "ess = []\n",
    "ess_n = []\n",
    "performances = []\n",
    "\n",
    "with pm.Model():\n",
    "    # Uniform priors on parameters\n",
    "    parameters = []\n",
    "    for i in range(nparam):\n",
    "        parameters.append(pm.Uniform('theta_' + str(i), lower=-3., upper=3.))\n",
    "\n",
    "    # Convert m and c to a tensor vector\n",
    "    theta = tt.as_tensor_variable(parameters)\n",
    "\n",
    "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
    "    pm.DensityDist('likelihood', lambda v: logl[-1](v), observed={'v': theta})\n",
    "\n",
    "    # Initialise an MLDA step method object, passing the subsampling rate and\n",
    "    # coarse models list\n",
    "    # Also initialise a Metropolis step method object\n",
    "    step_metropolis = pm.Metropolis(tune=tune, tune_interval=tune_interval, blocked=blocked)\n",
    "    step_mlda = pm.MLDA(subsampling_rate=nsub, coarse_models=coarse_models,\n",
    "                        tune=tune, tune_interval=tune_interval, base_blocked=blocked)\n",
    "\n",
    "    # Inference!\n",
    "    # Metropolis\n",
    "    t_start = time.time()\n",
    "    method_names.append(\"Metropolis\")\n",
    "    traces.append(pm.sample(draws=ndraws, step=step_metropolis,\n",
    "                            chains=nchains, tune=nburn,\n",
    "                            discard_tuned_samples=discard_tuning,\n",
    "                            random_seed=sampling_seed))\n",
    "    runtimes.append(time.time() - t_start)\n",
    "    \n",
    "    # MLDA\n",
    "    t_start = time.time()\n",
    "    method_names.append(\"MLDA\")\n",
    "    traces.append(pm.sample(draws=ndraws, step=step_mlda,\n",
    "                            chains=nchains, tune=nburn,\n",
    "                            discard_tuned_samples=discard_tuning,\n",
    "                            random_seed=sampling_seed))\n",
    "    runtimes.append(time.time() - t_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trace in enumerate(traces):\n",
    "    acc.append(trace.get_sampler_stats('accepted').mean())\n",
    "    ess.append(np.array(pm.ess(trace).to_array()))\n",
    "    ess_n.append(ess[i] / len(trace) / trace.nchains)\n",
    "    performances.append(ess[i] / runtimes[i])\n",
    "    print(f'\\nSampler {method_names[i]}: {len(trace)} drawn samples in each of '\n",
    "          f'{trace.nchains} chains.'\n",
    "          f'\\nRuntime: {runtimes[i]} seconds'\n",
    "          f'\\nAcceptance rate: {acc[i]}'\n",
    "          f'\\nESS list: {ess[i]}'\n",
    "          f'\\nNormalised ESS list: {ess_n[i]}'\n",
    "          f'\\nESS/sec: {performances[i]}')\n",
    "\n",
    "print(f\"\\nMLDA vs. Metropolis performance speedup in all dimensions (performance measured by ES/sec):\\n{np.array(performances[1]) / np.array(performances[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces[1].get_sampler_stats(\"base_scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show stats summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print true theta values and pymc3 sampling summary\n",
    "print(f\"\\nDetailed summaries and plots:\\nTrue parameters: {true_parameters}\")\n",
    "for i, trace in enumerate(traces):\n",
    "    print(f\"\\nSampler {method_names[i]}:\\n\", pm.stats.summary(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show traceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print true theta values and pymc3 sampling summary\n",
    "for i, trace in enumerate(traces):\n",
    "    pm.plots.traceplot(trace)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

import os
import numpy as np
import pymc3 as pm
import theano.tensor as tt
from model import Model
from itertools import product
import matplotlib.pyplot as plt
os.environ['OPENBLAS_NUM_THREADS'] = '1'  # Set environmental variable


def model_wrapper(my_model, theta, datapoints):
    """
    This solves a model given theta and returns the solution
    on the specified datapoints. Argument my_model is an object
    of class Model defined in model.py.
    """
    my_model.solve(theta)
    return my_model.get_data(datapoints)


def my_loglik(my_model, theta, datapoints, data, sigma):
    """
    This returns the log-likelihood of my_model given theta,
    datapoints, the observed data and sigma. It uses the
    model_wrapper function to do a model solve.
    """
    output = model_wrapper(my_model, theta, datapoints)
    return - (0.5 / sigma ** 2) * np.sum((output - data) ** 2)


class LogLike(tt.Op):
    """
    Theano Op that wraps the log-likelihood computation, necessary to
    pass "black-box" fenics code into pymc3.
    See https://docs.pymc.io/notebooks/blackbox_external_likelihood.html
    and https://docs.pymc.io/Advanced_usage_of_Theano_in_PyMC3.html for
    explanation.
    """

    # Specify what type of object will be passed and returned to the Op when it is
    # called. In our case we will be passing it a vector of values (the parameters
    # that define our model and a model object) and returning a single "scalar"
    # value (the log-likelihood)
    itypes = [tt.dvector]  # expects a vector of parameter values when called
    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)

    def __init__(self, my_model, loglike, data, x, sigma):
        """
        Initialise the Op with various things that our log-likelihood function
        requires. Below are the things that are needed in this particular
        example.

        Parameters
        ----------
        my_model:
            A Model object (defined in model.py) that contains the parameters
            and functions of out model.
        loglike:
            The log-likelihood function we've defined, in this example it is
            my_loglik.
        data:
            The "observed" data that our log-likelihood function takes in. These
            are the true data generated by the finest model in this example.
        x:
            The dependent variable (aka 'x') that our model requires. This is
            the datapoints in this example.
        sigma:
            The noise standard deviation that our function requires.
        """

        # add inputs as class attributes
        self.my_model = my_model
        self.likelihood = loglike
        self.data = data
        self.x = x
        self.sigma = sigma

    def perform(self, node, inputs, outputs):
        # the method that is used when calling the Op
        theta = inputs  # this will contain my variables

        # call the log-likelihood function
        logl = self.likelihood(self.my_model, theta, self.x, self.data, self.sigma)

        outputs[0][0] = np.array(logl) # output the log-likelihood


# PART 1: PARAMETERS
# Set the resolution of the multi-level models (from coarsest to finest)
# and the random field parameters.
resolutions = [(2, 2), (4, 4)]
field_mean = 0
field_stdev = 1
lamb_cov = 0.1
# Set the number of unknown parameters
mkl = 8
# Number of draws from the distribution
ndraws = 100
# Number of "burn-in points" (which we'll discard)
nburn = 20
# Set the sigma for inference
sigma = 0.01

# PART 2: GENERATE MODELS AND DATA
# Initialise model objects for all levels
my_models = []
for r in resolutions:
    my_models.append(Model(r, field_mean, field_stdev, mkl, lamb_cov))

# Solve finest model and plot transmissivity field and solution
np.random.seed(16643)
my_models[-1].solve()
my_models[-1].plot(transform_field=True)

# Save true parameters of finest model
true_parameters = my_models[-1].random_process.parameters

# Define the sampling points.
x_data = y_data = np.array([0.1, 0.3, 0.5, 0.7, 0.9])
datapoints = np.array(list(product(x_data, y_data)))

# Get data from the sampling points and perturb it with some noise.
noise = np.random.normal(0, 0.001, len(datapoints))

# Generate data from the finest model for use in pymc3 inference
data = model_wrapper(my_models[-1], true_parameters, datapoints) + noise

# Test the log-likelihood function with the finest model
my_loglik(my_models[-1], true_parameters, datapoints, data, sigma)

# create Theano Ops to wrap likelihoods of all model levels and store them in list
logl = []
for m in my_models:
    logl.append(LogLike(m, my_loglik, data, datapoints, sigma))


# PART 3: INFERENCE IN PYMC3
# Set up models in PyMC3 for each level - excluding finest model level
coarse_models = []
for j in range(len(my_models) - 1):
    with pm.Model() as model:
        # uniform priors on parameters
        parameters = []
        for i in range(mkl):
            parameters.append(pm.Uniform('theta_' + str(i), lower=-3., upper=3.))

        # convert m and c to a tensor vector
        theta = tt.as_tensor_variable(parameters)

        # use a DensityDist (use a lamdba function to "call" the Op)
        pm.DensityDist('likelihood', lambda v: logl[j](v), observed={'v': theta})

    coarse_models.append(model)

# Set up finest model and perform inference with PyMC3, using the MLDA algorithm
# and passing the coarse_models list created above.
with pm.Model():
    # uniform priors on parameters
    parameters = []
    for i in range(mkl):
        parameters.append(pm.Uniform('theta_' + str(i), lower=-3., upper=3.))

    # convert m and c to a tensor vector
    theta = tt.as_tensor_variable(parameters)

    # use a DensityDist (use a lamdba function to "call" the Op)
    pm.DensityDist('likelihood', lambda v: logl[-1](v), observed={'v': theta})

    # initialise an MLDA step method object, passing the subsampling rate and
    # coarse models list
    step_temp = pm.MLDA(subsampling_rate=5, coarse_models=coarse_models)

    # inference
    trace = pm.sample(ndraws, chains=1, cores=1, tune=nburn, step=step_temp, random_seed=1234)
    trace2 = pm.sample(ndraws, step=pm.Metropolis(), chains=1, cores=1, tune=nburn, random_seed=1234)

    # print true theta values and pymc3 sampling summary
    print(true_parameters)
    print(pm.stats.summary(trace))
    print(pm.stats.summary(trace2))

    # generate pymc3 traceplot
    pm.plots.traceplot(trace)
    pm.plots.traceplot(trace2)
    plt.show()


# PART 4: VALIDATION
# Evaluate MCMC sample mean for each unknown parameter (theta)
samples_mean = []
for i in range(mkl):
    samples_mean.append(trace['theta_'+str(i)].mean())

# Solve the finest model using the inferred theta means
# and plot transmissivity field and solution
my_models[-1].solve(true_parameters)
my_models[-1].plot(transform_field=True)
my_models[-1].solve(samples_mean)
my_models[-1].plot(transform_field=True)
